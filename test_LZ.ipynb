{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69809eb0-e5fe-4b07-99c6-f83dd43fbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import json, re, time\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "\n",
    "TRANSCRIPT_PATH = f\"psychs_transcripts\"\n",
    "\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "os.environ.setdefault(\"HF_HOME\", f\"/scratch/users/{os.getenv('USER','user')}/hf_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7408200",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "print(f\"[INFO] Loading model from: {MODEL_PATH}\")\n",
    "print(f\"[INFO] Device: {device}, DType: {dtype}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6e2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_files = [f for f in Path(TRANSCRIPT_PATH).glob(\"*.txt\") \n",
    "                   if \"prompt\" not in f.name and \"assessment\" not in f.name]\n",
    "target_ids = {\"BM97794\", \"ME39294\", \"NC30190\"}\n",
    "\n",
    "matches = [f for f in transcript_files if any(tid in f.name for tid in target_ids)]\n",
    "outputs_dir = Path(\"outputs_score\")\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffa74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: PrescientBM_BM97794_interviewAudioTranscript_psychs_day0001_session001_REDACTED.txt\n",
      "Saved: outputs_score/psychs_transcripts/PrescientBM_BM97794_interviewAudioTranscript_psychs_day0001_session001_REDACTED_assessment_Dom_v2_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0094_session004_REDACTED.txt\n",
      "Saved: outputs_score/psychs_transcripts/PrescientME_ME39294_interviewAudioTranscript_psychs_day0094_session004_REDACTED_assessment_Dom_v2_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0043_session002_REDACTED.txt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Both endpoints failed:\n\n/api/chat/completions -> 502 https://ai.create.kcl.ac.uk/api/chat/completions\nResponse: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html><head>\n<title>502 Proxy Error</title>\n</head><body>\n<h1>Proxy Error</h1>\n<p>The proxy server received an invalid\r\nresponse from an upstream server.<br />\r\nThe proxy server could not handle the request<p>Reason: <strong>Error reading from remote server</strong></p></p>\n</body></html>\n\n\n/v1/chat/completions -> Unexpected non-JSON response (content-type=text/html; charset=utf-8). First 500 chars: \r\\n\r\\n<!-- Copyright (C) Microsoft Corporation. All rights reserved. -->\r\\n<!DOCTYPE html>\r\\n<html dir=\"ltr\" class=\"\" lang=\"en\">\r\\n<head>\r\\n    <title>Sign in to your account</title>\r\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\r\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\r\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=2.0, user-scalable=yes\">\r\\n    <meta http-equiv=\"Pragma\" content=\"no-cache\">\r\\n    <meta http-equiv=\"Exp",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m prompt = prompt_template.format(transcript=transcript)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranscript_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m assessment = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or generate_assessment(prompt, model=\"qwen3\")\u001b[39;00m\n\u001b[32m     18\u001b[39m output_file = \u001b[38;5;28mstr\u001b[39m(transcript_file).replace(\u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_assessment_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m out_path = outputs_dir / output_file\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mchat\u001b[39m\u001b[34m(msg, model)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m         errors.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mBoth endpoints failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(errors))\n",
      "\u001b[31mRuntimeError\u001b[39m: Both endpoints failed:\n\n/api/chat/completions -> 502 https://ai.create.kcl.ac.uk/api/chat/completions\nResponse: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html><head>\n<title>502 Proxy Error</title>\n</head><body>\n<h1>Proxy Error</h1>\n<p>The proxy server received an invalid\r\nresponse from an upstream server.<br />\r\nThe proxy server could not handle the request<p>Reason: <strong>Error reading from remote server</strong></p></p>\n</body></html>\n\n\n/v1/chat/completions -> Unexpected non-JSON response (content-type=text/html; charset=utf-8). First 500 chars: \r\\n\r\\n<!-- Copyright (C) Microsoft Corporation. All rights reserved. -->\r\\n<!DOCTYPE html>\r\\n<html dir=\"ltr\" class=\"\" lang=\"en\">\r\\n<head>\r\\n    <title>Sign in to your account</title>\r\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\r\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\r\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, maximum-scale=2.0, user-scalable=yes\">\r\\n    <meta http-equiv=\"Pragma\" content=\"no-cache\">\r\\n    <meta http-equiv=\"Exp"
     ]
    }
   ],
   "source": [
    "\n",
    "# All at once\n",
    "prompt_name = 'Dom_v2'\n",
    "\n",
    "with open(f'prompt_{prompt_name}.txt', 'r') as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "\n",
    "# model = 'gemma3n:latest'\n",
    "\n",
    "for model in ['gemma3n:latest']:\n",
    "\n",
    "    for transcript_file in matches:\n",
    "        with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcript = f.read()\n",
    "\n",
    "        prompt = prompt_template.format(transcript=transcript)\n",
    "\n",
    "        print(f\"\\nProcessing: {transcript_file.name}\")\n",
    "        assessment = chat(prompt,model)  # or generate_assessment(prompt, model=\"qwen3\")\n",
    "\n",
    "        output_file = str(transcript_file).replace(\".txt\", f\"_assessment_{prompt_name}_{model}.txt\")\n",
    "        out_path = outputs_dir / output_file\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"File: {transcript_file.name}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            f.write(assessment)\n",
    "\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22639a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "_num = r'[-+]?(?:\\d+(?:\\.\\d+)?|\\.\\d+)(?:[eE][-+]?\\d+)?'  # int, float, .float, sci\n",
    "\n",
    "def _to_float(s: str) -> float:\n",
    "    s = s.replace(\",\", \"\")  # allow \"1,234.5\"\n",
    "    # s matches _num by construction, so float() is safe without try/except\n",
    "    return float(s)\n",
    "\n",
    "# use int?\n",
    "def extract_number(text: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Returns the first 'meaningful' number it finds:\n",
    "      1) percentage like '87%'  -> 87.0\n",
    "      2) fraction  like '4/6'   -> 4.0  (numerator)\n",
    "      3) plain number           -> value\n",
    "    If none found, returns None.\n",
    "    \"\"\"\n",
    "    # 1) percentage\n",
    "    m = re.search(rf'(?<!\\d)({_num})\\s*%', text)\n",
    "    if m:\n",
    "        return _to_float(m.group(1))\n",
    "\n",
    "    # 2) fraction (return the numerator as the number)\n",
    "    m = re.search(rf'({_num})\\s*/\\s*({_num})', text)\n",
    "    if m:\n",
    "        return _to_float(m.group(1))\n",
    "\n",
    "    # 3) plain number\n",
    "    m = re.search(rf'({_num})', text)\n",
    "    return _to_float(m.group(1)) if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7c3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: PrescientBM_BM97794_interviewAudioTranscript_psychs_day0001_session001_REDACTED.txt\n",
      "Estimated severity score: 3.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientBM_BM97794_interviewAudioTranscript_psychs_day0001_session001_REDACTED_assessment_Alejo_sev_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0094_session004_REDACTED.txt\n",
      "Estimated severity score: 2.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientME_ME39294_interviewAudioTranscript_psychs_day0094_session004_REDACTED_assessment_Alejo_sev_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0043_session002_REDACTED.txt\n",
      "Estimated severity score: 3.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientME_ME39294_interviewAudioTranscript_psychs_day0043_session002_REDACTED_assessment_Alejo_sev_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0038_session001_REDACTED.txt\n",
      "Estimated severity score: 2.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientME_ME39294_interviewAudioTranscript_psychs_day0038_session001_REDACTED_assessment_Alejo_sev_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientBM_BM97794_interviewAudioTranscript_psychs_day0069_session002_REDACTED.txt\n",
      "Estimated severity score: 3.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientBM_BM97794_interviewAudioTranscript_psychs_day0069_session002_REDACTED_assessment_Alejo_sev_gemma3n:latest.txt\n",
      "\n",
      "Processing: PronetNC_NC30190_interviewAudioTranscript_psychs_day0001_session001_REDACTED.txt\n",
      "Estimated severity score: 3.0\n",
      "Saved: outputs_score/psychs_transcripts/PronetNC_NC30190_interviewAudioTranscript_psychs_day0001_session001_REDACTED_assessment_Alejo_sev_gemma3n:latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Step by step\n",
    "\n",
    "# Severity\n",
    "#  \n",
    "model ='gemma3n:latest'\n",
    "prompt_name = 'Alejo'\n",
    "with open(f'prompt_{prompt_name}_sev.txt', 'r') as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "sev_scores = []\n",
    "for transcript_file in matches:\n",
    "    with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "\n",
    "    prompt = prompt_template.format(transcript=transcript)\n",
    "\n",
    "    print(f\"\\nProcessing: {transcript_file.name}\")\n",
    "    assessment = chat(prompt,model)  # or generate_assessment(prompt, model=\"qwen3\")\n",
    "    sev_score = extract_number(assessment)\n",
    "    sev_scores.append(sev_score)\n",
    "    print(f'Estimated severity score: {sev_score}')\n",
    "\n",
    "    output_file = str(transcript_file).replace(\".txt\", f\"_assessment_{prompt_name}_sev_{model}.txt\")\n",
    "    out_path = outputs_dir / output_file\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"File: {transcript_file.name}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(assessment)\n",
    "\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4ac329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: PrescientBM_BM97794_interviewAudioTranscript_psychs_day0001_session001_REDACTED.txt\n",
      "Estimated frequency score: 2.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientBM_BM97794_interviewAudioTranscript_psychs_day0001_session001_REDACTED_assessment_Alejo_freq_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0094_session004_REDACTED.txt\n",
      "Estimated frequency score: 1.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientME_ME39294_interviewAudioTranscript_psychs_day0094_session004_REDACTED_assessment_Alejo_freq_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0043_session002_REDACTED.txt\n",
      "Estimated frequency score: 3.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientME_ME39294_interviewAudioTranscript_psychs_day0043_session002_REDACTED_assessment_Alejo_freq_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientME_ME39294_interviewAudioTranscript_psychs_day0038_session001_REDACTED.txt\n",
      "Estimated frequency score: 40.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientME_ME39294_interviewAudioTranscript_psychs_day0038_session001_REDACTED_assessment_Alejo_freq_gemma3n:latest.txt\n",
      "\n",
      "Processing: PrescientBM_BM97794_interviewAudioTranscript_psychs_day0069_session002_REDACTED.txt\n",
      "Estimated frequency score: 2.0\n",
      "Saved: outputs_score/psychs_transcripts/PrescientBM_BM97794_interviewAudioTranscript_psychs_day0069_session002_REDACTED_assessment_Alejo_freq_gemma3n:latest.txt\n",
      "\n",
      "Processing: PronetNC_NC30190_interviewAudioTranscript_psychs_day0001_session001_REDACTED.txt\n",
      "Estimated frequency score: 2.0\n",
      "Saved: outputs_score/psychs_transcripts/PronetNC_NC30190_interviewAudioTranscript_psychs_day0001_session001_REDACTED_assessment_Alejo_freq_gemma3n:latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Step by step\n",
    "\n",
    "# Frequency\n",
    "\n",
    "with open(f'prompt_{prompt_name}_freq.txt', 'r') as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "freq_scores = []\n",
    "for transcript_file,sev_score in zip(matches,sev_scores):\n",
    "    with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "\n",
    "    prompt = prompt_template.format(transcript=transcript,sev_score=sev_score)\n",
    "\n",
    "    print(f\"\\nProcessing: {transcript_file.name}\")\n",
    "    assessment = chat(prompt,model)  # or generate_assessment(prompt, model=\"qwen3\")\n",
    "\n",
    "    output_file = str(transcript_file).replace(\".txt\", f\"_assessment_{prompt_name}_freq_{model}.txt\")\n",
    "    out_path = outputs_dir / output_file\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    freq_score = extract_number(assessment)\n",
    "    freq_scores.append(freq_score)\n",
    "    print(f'Estimated frequency score: {freq_score}')\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"File: {transcript_file.name}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(assessment)\n",
    "\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c9745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: PronetNC_NC30190_interviewAudioTranscript_psychs_day0001_session001_REDACTED.txt\n",
      "Saved: outputs_score/psychs_transcripts/PronetNC_NC30190_interviewAudioTranscript_psychs_day0001_session001_REDACTED_assessment_Alejo_sum_gemma3n:latest.txt\n"
     ]
    }
   ],
   "source": [
    "# Step by step\n",
    "\n",
    "# Summary\n",
    "\n",
    "with open(f'prompt_{prompt_name}_sum.txt', 'r') as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "\n",
    "for transcript_file,sev_score,freq_score in zip(matches,sev_scores,freq_scores):\n",
    "    with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "\n",
    "    prompt = prompt_template.format(transcript=transcript,sev_score=sev_score,freq_score=freq_score)\n",
    "\n",
    "    print(f\"\\nProcessing: {transcript_file.name}\")\n",
    "    assessment = chat(prompt,model)  # or generate_assessment(prompt, model=\"qwen3\")\n",
    "\n",
    "    output_file = str(transcript_file).replace(\".txt\", f\"_assessment_{prompt_name}_sum_{model}.txt\")\n",
    "    out_path = outputs_dir / output_file\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"File: {transcript_file.name}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(assessment)\n",
    "\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cbd2d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwen3',\n",
       " 'qwen3',\n",
       " 'qwen3',\n",
       " 'gpt-oss:20b',\n",
       " 'gpt-oss:20b',\n",
       " 'gpt-oss:20b',\n",
       " 'gemma3n:latest',\n",
       " 'gemma3n:latest',\n",
       " 'gemma3n:latest',\n",
       " 'gemma3:1b',\n",
       " 'gemma3:1b',\n",
       " 'gemma3:1b']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BM97794 (Severity = 3; Frequency = 3)\n",
    "# ME39294 day 38 (Severity = 4; Frequency = 6)\n",
    "# NC30190 (Severity = 6; Frequency = 5)\n",
    "collect_ids(list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbf97e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nemo.collections.asr as nemo_asr \n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset, Audio\n",
    "from openvoice.api import ToneColorConverter\n",
    "from melo.api import TTS\n",
    "\n",
    "BASE_PATH = Path(\"/mnt/sdg/tzhu/llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wav_path = BASE_PATH / \"sample.flac\"\n",
    "\n",
    "if not wav_path.exists():\n",
    "    print(\"Downloading tiny demo clip (no decoding)…\")\n",
    "    ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "    ds = ds.cast_column(\"audio\", Audio(decode=False))  # <-- critical: returns file path/bytes, no torchcodec\n",
    "    rec = ds[0][\"audio\"]  # {'bytes': b'...', 'path': '1272-128104-0000.flac'}\n",
    "    with open(wav_path, \"wb\") as f:\n",
    "        f.write(rec[\"bytes\"])\n",
    "    print(f\"✅ Saved sample audio to {wav_path}\")\n",
    "\n",
    "# ---------- run NeMo ASR ----------\n",
    "print(\"📥 Loading nvidia/canary-1b-flash…\")\n",
    "asr = nemo_asr.models.EncDecMultiTaskModel.from_pretrained(\"nvidia/canary-1b-flash\")\n",
    "\n",
    "print(\"🗣️ Transcribing…\")\n",
    "out = asr.transcribe([str(wav_path)], batch_size=1)\n",
    "transcript = out[0].text if (out and hasattr(out[0], \"text\")) else out[0]\n",
    "print(\"✏️ Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- LLaMA 4 Scout 17B-16E --------\n",
    "llama_model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "llama_local_path = f\"{BASE_PATH}/{llama_model_id}\"\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_local_path, \n",
    "    local_files_only=True\n",
    ")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_local_path,  \n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    local_files_only=True  \n",
    ")\n",
    "# Test LLaMA\n",
    "llama_pipe = pipeline(\"text-generation\", model=llama_model, tokenizer=llama_tokenizer)\n",
    "print(\"LLaMA test:\")\n",
    "print(llama_pipe(\"Summarise this candidate's strengths based on the interview notes: \", max_new_tokens=50)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "ckpt_converter = f\"{BASE_PATH}/OpenVoice/checkpoints_v2/converter\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_dir = 'outputs_v2'\n",
    "\n",
    "# Load converter\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')\n",
    "\n",
    "# Create output directory\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Load TTS for generating test audio\n",
    "tts_model = TTS(language='EN', device=device)\n",
    "\n",
    "# Generate test audio\n",
    "text = \"Hello, this is a test of OpenVoice version two to generate audio.\"\n",
    "src_path = f\"{output_dir}/test_tts.wav\"\n",
    "tts_model.tts_to_file(text, speaker_id=0, output_path=src_path)\n",
    "print(f\"Generated TTS: {src_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tzhu/miniconda3/envs/llm/lib/python3.10/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/tzhu/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nemo.collections.asr as nemo_asr \n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset, Audio\n",
    "from openvoice.api import ToneColorConverter\n",
    "from melo.api import TTS\n",
    "\n",
    "BASE_PATH = Path(\"/mnt/sdg/tzhu/llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading nvidia/canary-1b-flashâ€¦\n",
      "[NeMo I 2025-08-10 00:18:37 mixins:205] _setup_tokenizer: detected an aggregate tokenizer\n",
      "[NeMo I 2025-08-10 00:18:37 mixins:344] Tokenizer SentencePieceTokenizer initialized with 1152 tokens\n",
      "[NeMo I 2025-08-10 00:18:37 mixins:344] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-08-10 00:18:37 mixins:344] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-08-10 00:18:37 mixins:344] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-08-10 00:18:37 mixins:344] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n",
      "[NeMo I 2025-08-10 00:18:37 aggregate_tokenizer:73] Aggregate vocab size: 5248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-10 00:18:37 modelPT:181] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    prompt_format: canary2\n",
      "    max_tps: 25\n",
      "    max_duration: 40.0\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins:\n",
      "    - - 3.971\n",
      "      - 30\n",
      "    - - 3.971\n",
      "      - 48\n",
      "    - - 4.973\n",
      "      - 37\n",
      "    - - 4.973\n",
      "      - 60\n",
      "    - - 5.85\n",
      "      - 42\n",
      "    - - 5.85\n",
      "      - 71\n",
      "    - - 6.56\n",
      "      - 46\n",
      "    - - 6.56\n",
      "      - 79\n",
      "    - - 7.32\n",
      "      - 49\n",
      "    - - 7.32\n",
      "      - 88\n",
      "    - - 8.19\n",
      "      - 54\n",
      "    - - 8.19\n",
      "      - 99\n",
      "    - - 8.88\n",
      "      - 61\n",
      "    - - 8.88\n",
      "      - 107\n",
      "    - - 9.76\n",
      "      - 66\n",
      "    - - 9.76\n",
      "      - 118\n",
      "    - - 10.56\n",
      "      - 72\n",
      "    - - 10.56\n",
      "      - 127\n",
      "    - - 11.214\n",
      "      - 76\n",
      "    - - 11.214\n",
      "      - 135\n",
      "    - - 11.867\n",
      "      - 79\n",
      "    - - 11.867\n",
      "      - 143\n",
      "    - - 12.53\n",
      "      - 82\n",
      "    - - 12.53\n",
      "      - 151\n",
      "    - - 13.08\n",
      "      - 87\n",
      "    - - 13.08\n",
      "      - 157\n",
      "    - - 13.62\n",
      "      - 91\n",
      "    - - 13.62\n",
      "      - 164\n",
      "    - - 14.16\n",
      "      - 93\n",
      "    - - 14.16\n",
      "      - 170\n",
      "    - - 14.7\n",
      "      - 96\n",
      "    - - 14.7\n",
      "      - 177\n",
      "    - - 15.2\n",
      "      - 99\n",
      "    - - 15.2\n",
      "      - 183\n",
      "    - - 15.68\n",
      "      - 101\n",
      "    - - 15.68\n",
      "      - 189\n",
      "    - - 16.135\n",
      "      - 102\n",
      "    - - 16.135\n",
      "      - 194\n",
      "    - - 16.67\n",
      "      - 105\n",
      "    - - 16.67\n",
      "      - 201\n",
      "    - - 17.197\n",
      "      - 108\n",
      "    - - 17.197\n",
      "      - 207\n",
      "    - - 17.73\n",
      "      - 111\n",
      "    - - 17.73\n",
      "      - 213\n",
      "    - - 18.2\n",
      "      - 114\n",
      "    - - 18.2\n",
      "      - 219\n",
      "    - - 18.69\n",
      "      - 117\n",
      "    - - 18.69\n",
      "      - 225\n",
      "    - - 19.153\n",
      "      - 120\n",
      "    - - 19.153\n",
      "      - 230\n",
      "    - - 19.63\n",
      "      - 123\n",
      "    - - 19.63\n",
      "      - 236\n",
      "    - - 20.44\n",
      "      - 122\n",
      "    - - 20.44\n",
      "      - 246\n",
      "    - - 32.567\n",
      "      - 174\n",
      "    - - 32.567\n",
      "      - 391\n",
      "    - - 36.587\n",
      "      - 227\n",
      "    - - 36.587\n",
      "      - 440\n",
      "    - - 40.0\n",
      "      - 253\n",
      "    - - 40.0\n",
      "      - 480\n",
      "    bucket_batch_size:\n",
      "    - 273\n",
      "    - 248\n",
      "    - 217\n",
      "    - 190\n",
      "    - 190\n",
      "    - 161\n",
      "    - 165\n",
      "    - 140\n",
      "    - 149\n",
      "    - 126\n",
      "    - 134\n",
      "    - 113\n",
      "    - 120\n",
      "    - 100\n",
      "    - 109\n",
      "    - 93\n",
      "    - 99\n",
      "    - 86\n",
      "    - 94\n",
      "    - 79\n",
      "    - 86\n",
      "    - 72\n",
      "    - 82\n",
      "    - 69\n",
      "    - 79\n",
      "    - 66\n",
      "    - 74\n",
      "    - 63\n",
      "    - 71\n",
      "    - 61\n",
      "    - 68\n",
      "    - 57\n",
      "    - 66\n",
      "    - 55\n",
      "    - 66\n",
      "    - 54\n",
      "    - 63\n",
      "    - 52\n",
      "    - 61\n",
      "    - 50\n",
      "    - 59\n",
      "    - 48\n",
      "    - 57\n",
      "    - 46\n",
      "    - 55\n",
      "    - 45\n",
      "    - 53\n",
      "    - 43\n",
      "    - 51\n",
      "    - 41\n",
      "    - 50\n",
      "    - 39\n",
      "    - 50\n",
      "    - 39\n",
      "    - 28\n",
      "    - 21\n",
      "    - 24\n",
      "    - 18\n",
      "    - 21\n",
      "    - 17\n",
      "    bucket_buffer_size: 40000\n",
      "    shuffle_buffer_size: 10000\n",
      "    concurrent_bucketing: false\n",
      "    augmentor: null\n",
      "    \n",
      "[NeMo W 2025-08-10 00:18:37 modelPT:188] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    prompt_format: canary2\n",
      "    manifest_filepath: /data/ASR/en/librispeech/test-other.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    lang_field: target_lang\n",
      "    tarred_audio_filepaths: null\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-10 00:18:37 features:305] PADDING: 0\n",
      "[NeMo I 2025-08-10 00:18:48 save_restore_connector:282] Model EncDecMultiTaskModel was successfully restored from /home/tzhu/.cache/huggingface/hub/models--nvidia--canary-1b-flash/snapshots/652da3a37d8bd7e23599359e6c3f857c80cfb657/canary-1b-flash.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-10 00:18:48 dataloader:732] The following configuration keys are ignored by Lhotse dataloader: trim_silence\n",
      "[NeMo W 2025-08-10 00:18:48 dataloader:479] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—£ï¸ Transcribingâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 1it [00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœï¸ Transcript: mister Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wav_path = BASE_PATH / \"sample.flac\"\n",
    "\n",
    "if not wav_path.exists():\n",
    "    print(\"Downloading tiny demo clip (no decoding)â€¦\")\n",
    "    ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "    ds = ds.cast_column(\"audio\", Audio(decode=False))  # <-- critical: returns file path/bytes, no torchcodec\n",
    "    rec = ds[0][\"audio\"]  # {'bytes': b'...', 'path': '1272-128104-0000.flac'}\n",
    "    with open(wav_path, \"wb\") as f:\n",
    "        f.write(rec[\"bytes\"])\n",
    "    print(f\"âœ… Saved sample audio to {wav_path}\")\n",
    "\n",
    "# ---------- run NeMo ASR ----------\n",
    "print(\"ðŸ“¥ Loading nvidia/canary-1b-flashâ€¦\")\n",
    "asr = nemo_asr.models.EncDecMultiTaskModel.from_pretrained(\"nvidia/canary-1b-flash\")\n",
    "\n",
    "print(\"ðŸ—£ï¸ Transcribingâ€¦\")\n",
    "out = asr.transcribe([str(wav_path)], batch_size=1)\n",
    "transcript = out[0].text if (out and hasattr(out[0], \"text\")) else out[0]\n",
    "print(\"âœï¸ Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:24<00:00,  1.68s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA test:\n",
      "Summarise this candidate's strengths based on the interview notes: 1. What is the best strength for the candidate? What makes you think that? 2. What is the best strength for the candidate? What makes you think that? 3. What is the best strength for the candidate? What makes you\n"
     ]
    }
   ],
   "source": [
    "# -------- LLaMA 4 Scout 17B-16E --------\n",
    "llama_model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "\n",
    "llama_local_path = f\"{BASE_PATH}/{llama_model_id}\"\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_local_path, \n",
    "    local_files_only=True\n",
    ")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_local_path,  \n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    local_files_only=True  \n",
    ")\n",
    "# Test LLaMA\n",
    "llama_pipe = pipeline(\"text-generation\", model=llama_model, tokenizer=llama_tokenizer)\n",
    "print(\"LLaMA test:\")\n",
    "print(llama_pipe(\"Summarise this candidate's strengths based on the interview notes: \", max_new_tokens=50)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint '/mnt/sdg/tzhu/llm/OpenVoice/checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      " > Text split to sentences.\n",
      "Hello, this is a test of OpenVoice version two to generate audio.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated TTS: outputs_v2/test_tts.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "ckpt_converter = f\"{BASE_PATH}/OpenVoice/checkpoints_v2/converter\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_dir = 'outputs_v2'\n",
    "\n",
    "# Load converter\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')\n",
    "\n",
    "# Create output directory\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Load TTS for generating test audio\n",
    "tts_model = TTS(language='EN', device=device)\n",
    "\n",
    "# Generate test audio\n",
    "text = \"Hello, this is a test of OpenVoice version two to generate audio.\"\n",
    "src_path = f\"{output_dir}/test_tts.wav\"\n",
    "tts_model.tts_to_file(text, speaker_id=0, output_path=src_path)\n",
    "print(f\"Generated TTS: {src_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
